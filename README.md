Chunking Evaluation for Question Answering
This repository contains code for evaluating different chunking strategies used in question-answering systems. The evaluation framework includes generating question-answer datasets from PDF documents, retrieving relevant chunks using embedding models, and calculating performance metrics.

Usage
Data Generation:

Place your PDF documents in the pdfs directory.
Run the main.py script with the PDF_DIR parameter set to the path of the pdfs directory.
This will generate question-answer datasets for each PDF using the different chunking strategies and save them in the output_dir directory.
Evaluation:

Run the main.py script again.
This will evaluate the performance of each chunking strategy on the generated datasets and save the results in a CSV file named "chunker_metrics.csv".
Analysis:

The analyze_chunk_lengths function in analysis.py can be used to analyze the distribution of chunk lengths for each chunker.
The count_chunks function in analysis.py can be used to count the number of chunks generated by each chunker for each document.
Key Files

main.py: The main script that orchestrates the data generation, evaluation, and analysis.
chunker.py: Defines the different chunking strategies.
data_generation.py: Generates question-answer datasets for each PDF.
evaluation.py: Evaluates the performance of chunking strategies.
analysis.py: Provides functions for analyzing the chunk lengths and counting the number of chunks.
Dependencies

Python 3.8+
numpy
pandas
matplotlib
langchain
fitz
Installation

Clone the repository.
Create a virtual environment and activate it.
Install the required dependencies using pip: pip install -r requirements.txt.   
Usage Examples

To generate question-answer datasets for all PDF files in the pdfs directory and evaluate the performance of each chunker:


```
python main.py
```

To analyze the chunk length distribution for a specific chunker:

```
from analysis import analyze_chunk_lengths

chunker_name = "RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)"
analyze_chunk_lengths(f"output_dir/{chunker_name}")
```

To count the number of chunks generated by each chunker for each document:

```
from analysis import count_chunks

df_chunks = count_chunks(output_dir)
print(df_chunks)
```
Contributions

Contributions are welcome! If you find any issues or have suggestions for improvements, please feel free to open an issue or submit a pull request.   

License

This code is released under the MIT License. See the LICENSE file for more details.   


Sources and related content
